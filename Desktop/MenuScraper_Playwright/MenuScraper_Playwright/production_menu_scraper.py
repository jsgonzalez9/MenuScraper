#!/usr/bin/env python3
"""
Production Menu Scraper - Final Implementation
Combines successful elements from Enhanced Scraper with production-ready features
Target: 50%+ success rate with robust error handling and practical deployment
"""

import asyncio
import json
import logging
import random
import re
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from urllib.parse import urljoin, urlparse

try:
    import requests
except ImportError:
    print("‚ùå Requests not installed. Run: pip install requests")
    requests = None

try:
    from playwright.async_api import async_playwright, Browser, Page
except ImportError:
    print("‚ùå Playwright not installed. Run: pip install playwright")
    exit(1)

try:
    from bs4 import BeautifulSoup
except ImportError:
    print("‚ùå BeautifulSoup not installed. Run: pip install beautifulsoup4")
    exit(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('production_scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ProductionMenuScraper:
    """Production-ready menu scraper with enhanced success rate and reliability"""
    
    def __init__(self, headless: bool = True, timeout: int = 30000, ocr_enabled: bool = False):
        self.headless = headless
        self.timeout = timeout
        self.ocr_enabled = ocr_enabled
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        
        # Enhanced extraction patterns based on successful Enhanced Scraper
        self.menu_selectors = [
            # Yelp-specific selectors (proven successful)
            'div[data-testid="menu-item"]',
            'div.menu-item',
            'div.menu-section-item',
            'div[class*="menu"][class*="item"]',
            
            # Generic menu selectors
            '.menu-item', '.menu-entry', '.dish', '.food-item',
            '[class*="menu-item"]', '[class*="dish"]', '[class*="food"]',
            'li[class*="menu"]', 'div[class*="menu"]',
            
            # Structured data selectors
            '[itemtype*="MenuItem"]', '[itemtype*="Food"]',
            '.menu .item', '.menu-list .item', '.food-menu .item'
        ]
        
        # Allergen detection patterns
        self.allergen_patterns = {
            'gluten': r'\b(gluten|wheat|flour|bread|pasta|cereal)\b',
            'dairy': r'\b(milk|cheese|butter|cream|yogurt|dairy)\b',
            'nuts': r'\b(nuts?|almond|walnut|pecan|cashew|peanut)\b',
            'shellfish': r'\b(shrimp|crab|lobster|shellfish|seafood)\b',
            'eggs': r'\b(egg|eggs|mayonnaise)\b',
            'soy': r'\b(soy|soybean|tofu|tempeh)\b',
            'fish': r'\b(fish|salmon|tuna|cod|halibut)\b',
            'sesame': r'\b(sesame|tahini)\b'
        }
        
        # Price extraction patterns
        self.price_patterns = [
            r'\$\d+\.\d{2}',  # $12.99
            r'\$\d+',         # $12
            r'\d+\.\d{2}',    # 12.99
            r'Price:\s*\$?\d+(?:\.\d{2})?'  # Price: $12.99
        ]
        
        # OCR and image processing settings
        self.image_selectors = [
            # Menu image selectors
            'img[alt*="menu"]',
            'img[src*="menu"]',
            'img[class*="menu"]',
            '.menu img',
            '.menu-image img',
            '[class*="menu"] img',
            
            # Food/dish images that might contain text
            'img[alt*="food"]',
            'img[alt*="dish"]',
            'img[class*="food"]',
            'img[class*="dish"]',
            
            # Generic image selectors for potential menu content
            '.restaurant img',
            '.content img',
            'main img'
        ]
    
    async def setup_browser(self) -> bool:
        """Setup browser with stealth configuration"""
        try:
            playwright = await async_playwright().start()
            
            # Enhanced browser setup for better success rate
            self.browser = await playwright.chromium.launch(
                headless=self.headless,
                args=[
                    '--no-sandbox',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-features=VizDisplayCompositor',
                    '--disable-web-security',
                    '--disable-features=TranslateUI',
                    '--disable-ipc-flooding-protection',
                    '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                ]
            )
            
            # Create page with enhanced settings
            self.page = await self.browser.new_page()
            
            # Set realistic viewport
            await self.page.set_viewport_size({"width": 1920, "height": 1080})
            
            # Add stealth scripts
            await self.page.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined,
                });
                
                Object.defineProperty(navigator, 'plugins', {
                    get: () => [1, 2, 3, 4, 5],
                });
                
                Object.defineProperty(navigator, 'languages', {
                    get: () => ['en-US', 'en'],
                });
            """)
            
            logger.info("‚úÖ Browser setup completed successfully")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Browser setup failed: {e}")
            return False
    
    async def navigate_with_retry(self, url: str, max_retries: int = 3) -> bool:
        """Navigate to URL with retry logic and bot detection handling"""
        for attempt in range(max_retries):
            try:
                logger.info(f"üåê Navigating to {url} (attempt {attempt + 1}/{max_retries})")
                
                # Random delay to appear more human-like
                await asyncio.sleep(random.uniform(1, 3))
                
                # Navigate with timeout
                response = await self.page.goto(
                    url, 
                    wait_until='domcontentloaded',
                    timeout=self.timeout
                )
                
                if response and response.status == 200:
                    # Wait for content to load
                    await asyncio.sleep(random.uniform(2, 4))
                    
                    # Check for bot detection
                    content = await self.page.content()
                    if self._is_bot_detected(content):
                        logger.warning(f"ü§ñ Bot detection on attempt {attempt + 1}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(random.uniform(5, 10))
                            continue
                        return False
                    
                    logger.info("‚úÖ Navigation successful")
                    return True
                else:
                    logger.warning(f"‚ö†Ô∏è HTTP {response.status if response else 'No response'}")
                    
            except Exception as e:
                logger.error(f"‚ùå Navigation attempt {attempt + 1} failed: {e}")
                
            if attempt < max_retries - 1:
                await asyncio.sleep(random.uniform(3, 7))
        
        return False
    
    def _is_bot_detected(self, content: str) -> bool:
        """Check if bot detection is present"""
        bot_indicators = [
            "robot", "captcha", "verification", "blocked",
            "access denied", "suspicious activity",
            "please verify", "security check"
        ]
        content_lower = content.lower()
        return any(indicator in content_lower for indicator in bot_indicators)
    
    async def extract_menu_items(self, url: str) -> Dict[str, Any]:
        """Extract menu items with enhanced strategies"""
        start_time = time.time()
        result = {
            'url': url,
            'success': False,
            'items': [],
            'total_items': 0,
            'processing_time': 0,
            'extraction_method': None,
            'allergen_summary': {},
            'price_coverage': 0,
            'menu_image_urls': [],
            'ocr_texts': [],
            'error': None
        }
        
        try:
            # Navigate to the page
            if not await self.navigate_with_retry(url):
                result['error'] = 'Navigation failed'
                return result
            
            # Get page content
            content = await self.page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            # Try multiple extraction strategies
            items = []
            extraction_method = None
            
            # Strategy 1: CSS Selectors (most successful in Enhanced Scraper)
            for selector in self.menu_selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    if elements and len(elements) > 0:
                        logger.info(f"üìã Found {len(elements)} items with selector: {selector}")
                        
                        for element in elements[:20]:  # Limit to prevent overwhelming
                            item_data = await self._extract_item_data(element)
                            if item_data and self._is_valid_menu_item(item_data):
                                items.append(item_data)
                        
                        if items:
                            extraction_method = f'css_selector: {selector}'
                            break
                            
                except Exception as e:
                    logger.debug(f"Selector {selector} failed: {e}")
                    continue
            
            # Strategy 2: Text Pattern Extraction (fallback)
            if not items:
                items = self._extract_by_text_patterns(soup)
                if items:
                    extraction_method = 'text_patterns'
            
            # Strategy 3: Structured Data (JSON-LD, microdata)
            if not items:
                items = self._extract_structured_data(soup)
                if items:
                    extraction_method = 'structured_data'
            
            # Strategy 4: OCR from menu images (if text extraction insufficient)
            menu_image_urls = []
            ocr_texts = []
            if not items or len(items) < 3:  # Try OCR if we have few items
                try:
                    menu_image_urls = await self._find_menu_image_urls()
                    if menu_image_urls:
                        logger.info(f"üñºÔ∏è Found {len(menu_image_urls)} potential menu images")
                        
                        for img_url in menu_image_urls[:3]:  # Limit to 3 images
                            ocr_text = await self._process_image_with_ocr(img_url)
                            if ocr_text:
                                ocr_texts.append(ocr_text)
                                # Extract items from OCR text
                                ocr_items = self._extract_items_from_ocr_text(ocr_text)
                                if ocr_items:
                                    items.extend(ocr_items)
                                    if not extraction_method:
                                        extraction_method = 'ocr_extraction'
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è OCR processing failed: {e}")
            
            # Process results
            if items:
                # Remove duplicates and clean data
                unique_items = self._deduplicate_items(items)
                
                # Add allergen detection
                for item in unique_items:
                    item['allergens'] = self._detect_allergens(item.get('description', '') + ' ' + item.get('name', ''))
                
                result.update({
                    'success': True,
                    'items': unique_items,
                    'total_items': len(unique_items),
                    'extraction_method': extraction_method,
                    'allergen_summary': self._summarize_allergens(unique_items),
                    'price_coverage': self._calculate_price_coverage(unique_items),
                    'menu_image_urls': menu_image_urls,
                    'ocr_texts': ocr_texts
                })
                
                logger.info(f"‚úÖ Successfully extracted {len(unique_items)} menu items")
            else:
                result['error'] = 'No menu items found'
                logger.warning("‚ö†Ô∏è No menu items extracted")
                
        except Exception as e:
            result['error'] = str(e)
            logger.error(f"‚ùå Extraction failed: {e}")
        
        finally:
            result['processing_time'] = round(time.time() - start_time, 2)
        
        return result
    
    async def _extract_item_data(self, element) -> Optional[Dict[str, Any]]:
        """Extract data from a menu item element"""
        try:
            # Get text content
            text_content = await element.text_content()
            if not text_content or len(text_content.strip()) < 3:
                return None
            
            # Get HTML for more detailed extraction
            html_content = await element.inner_html()
            
            # Extract name (usually the first line or prominent text)
            name = self._extract_item_name(text_content)
            
            # Extract price
            price = self._extract_price(text_content)
            
            # Extract description
            description = self._extract_description(text_content, name)
            
            return {
                'name': name,
                'description': description,
                'price': price,
                'raw_text': text_content.strip(),
                'allergens': []  # Will be populated later
            }
            
        except Exception as e:
            logger.debug(f"Item extraction failed: {e}")
            return None
    
    def _extract_item_name(self, text: str) -> str:
        """Extract item name from text"""
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        if lines:
            # Usually the first non-empty line is the name
            name = lines[0]
            # Remove price from name if present
            name = re.sub(r'\$\d+(?:\.\d{2})?', '', name).strip()
            return name[:100]  # Limit length
        return "Unknown Item"
    
    def _extract_price(self, text: str) -> Optional[str]:
        """Extract price from text"""
        for pattern in self.price_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(0)
        return None
    
    def _extract_description(self, text: str, name: str) -> str:
        """Extract description from text"""
        # Remove the name and price to get description
        description = text.replace(name, '', 1)
        
        # Remove price
        for pattern in self.price_patterns:
            description = re.sub(pattern, '', description)
        
        # Clean up
        description = re.sub(r'\s+', ' ', description).strip()
        return description[:500]  # Limit length
    
    def _extract_by_text_patterns(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """Extract menu items using text patterns"""
        items = []
        
        # Look for text that looks like menu items
        text_elements = soup.find_all(text=True)
        
        for text in text_elements:
            if text and len(text.strip()) > 10:
                # Check if text contains price patterns (good indicator of menu items)
                if any(re.search(pattern, text) for pattern in self.price_patterns):
                    item = {
                        'name': self._extract_item_name(text),
                        'description': self._extract_description(text, ''),
                        'price': self._extract_price(text),
                        'raw_text': text.strip(),
                        'allergens': []
                    }
                    if self._is_valid_menu_item(item):
                        items.append(item)
        
        return items[:10]  # Limit results
    
    def _extract_structured_data(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """Extract menu items from structured data"""
        items = []
        
        # Look for JSON-LD structured data
        scripts = soup.find_all('script', type='application/ld+json')
        for script in scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, dict) and 'hasMenu' in data:
                    # Process restaurant menu data
                    menu_items = self._process_structured_menu(data)
                    items.extend(menu_items)
            except (json.JSONDecodeError, KeyError):
                continue
        
        # Look for microdata
        microdata_items = soup.find_all(attrs={'itemtype': re.compile(r'MenuItem|Food')})
        for item in microdata_items:
            menu_item = self._process_microdata_item(item)
            if menu_item:
                items.append(menu_item)
        
        return items
    
    def _process_structured_menu(self, data: Dict) -> List[Dict[str, Any]]:
        """Process structured menu data"""
        items = []
        # Implementation would depend on the specific structured data format
        # This is a placeholder for structured data processing
        return items
    
    def _process_microdata_item(self, element) -> Optional[Dict[str, Any]]:
        """Process microdata menu item"""
        try:
            name_elem = element.find(attrs={'itemprop': 'name'})
            desc_elem = element.find(attrs={'itemprop': 'description'})
            price_elem = element.find(attrs={'itemprop': 'price'})
            
            if name_elem:
                return {
                    'name': name_elem.get_text(strip=True),
                    'description': desc_elem.get_text(strip=True) if desc_elem else '',
                    'price': price_elem.get_text(strip=True) if price_elem else None,
                    'raw_text': element.get_text(strip=True),
                    'allergens': []
                }
        except Exception:
            pass
        return None
    
    def _is_valid_menu_item(self, item: Dict[str, Any]) -> bool:
        """Validate if extracted data represents a valid menu item"""
        if not item or not item.get('name'):
            return False
        
        name = item['name'].lower()
        
        # Filter out non-menu items
        invalid_patterns = [
            'submit', 'button', 'click', 'menu', 'section',
            'category', 'view', 'more', 'see', 'robot', 'verify'
        ]
        
        if any(pattern in name for pattern in invalid_patterns):
            return False
        
        # Must have reasonable length
        if len(item['name']) < 2 or len(item['name']) > 100:
            return False
        
        return True
    
    def _deduplicate_items(self, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove duplicate menu items"""
        seen_names = set()
        unique_items = []
        
        for item in items:
            name_key = item['name'].lower().strip()
            if name_key not in seen_names:
                seen_names.add(name_key)
                unique_items.append(item)
        
        return unique_items
    
    def _detect_allergens(self, text: str) -> List[str]:
        """Detect allergens in menu item text"""
        allergens = []
        text_lower = text.lower()
        
        for allergen, pattern in self.allergen_patterns.items():
            if re.search(pattern, text_lower):
                allergens.append(allergen)
        
        return allergens
    
    def _summarize_allergens(self, items: List[Dict[str, Any]]) -> Dict[str, int]:
        """Summarize allergen occurrences across all items"""
        allergen_counts = {}
        
        for item in items:
            for allergen in item.get('allergens', []):
                allergen_counts[allergen] = allergen_counts.get(allergen, 0) + 1
        
        return allergen_counts
    
    def _calculate_price_coverage(self, items: List[Dict[str, Any]]) -> float:
        """Calculate percentage of items with price information"""
        if not items:
            return 0.0
        
        items_with_price = sum(1 for item in items if item.get('price'))
        return round((items_with_price / len(items)) * 100, 1)
    
    async def _find_menu_image_urls(self) -> List[str]:
        """Find potential menu image URLs on the page"""
        image_urls = []
        
        try:
            for selector in self.image_selectors:
                try:
                    img_elements = await self.page.query_selector_all(selector)
                    
                    for img_element in img_elements:
                        # Get image source
                        src = await img_element.get_attribute('src')
                        if src:
                            # Convert relative URLs to absolute
                            if src.startswith('//'):
                                src = 'https:' + src
                            elif src.startswith('/'):
                                src = urljoin(self.page.url, src)
                            
                            # Filter for likely menu images
                            if self._is_likely_menu_image(src):
                                image_urls.append(src)
                                
                        # Also check srcset for higher quality images
                        srcset = await img_element.get_attribute('srcset')
                        if srcset:
                            urls = re.findall(r'(https?://[^\s,]+)', srcset)
                            for url in urls:
                                if self._is_likely_menu_image(url):
                                    image_urls.append(url)
                                    
                except Exception as e:
                    logger.debug(f"Image selector {selector} failed: {e}")
                    continue
            
            # Remove duplicates while preserving order
            unique_urls = []
            seen = set()
            for url in image_urls:
                if url not in seen:
                    seen.add(url)
                    unique_urls.append(url)
            
            logger.info(f"üì∏ Found {len(unique_urls)} potential menu image URLs")
            return unique_urls[:5]  # Limit to 5 images
            
        except Exception as e:
            logger.error(f"‚ùå Image URL extraction failed: {e}")
            return []
    
    def _is_likely_menu_image(self, url: str) -> bool:
        """Check if image URL is likely to contain menu content"""
        if not url or len(url) < 10:
            return False
        
        url_lower = url.lower()
        
        # Positive indicators
        menu_indicators = [
            'menu', 'food', 'dish', 'restaurant', 'cuisine',
            'plate', 'meal', 'dining', 'kitchen'
        ]
        
        # Negative indicators
        exclude_indicators = [
            'logo', 'icon', 'avatar', 'profile', 'banner',
            'header', 'footer', 'background', 'bg',
            'thumb', 'small', 'tiny', '16x16', '32x32'
        ]
        
        # Check for positive indicators
        has_positive = any(indicator in url_lower for indicator in menu_indicators)
        
        # Check for negative indicators
        has_negative = any(indicator in url_lower for indicator in exclude_indicators)
        
        # Must have positive indicator and no negative indicators
        return has_positive and not has_negative
    
    async def _process_image_with_ocr(self, image_url: str) -> Optional[str]:
        """Process image with OCR to extract text"""
        try:
            if not self.ocr_enabled or not requests:
                return None
            
            logger.info(f"üîç Processing image with OCR: {image_url[:100]}...")
            
            # Download image
            response = requests.get(image_url, timeout=10, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            
            if response.status_code != 200:
                logger.warning(f"‚ö†Ô∏è Failed to download image: {response.status_code}")
                return None
            
            # Check image size (avoid processing very large images)
            if len(response.content) > 5 * 1024 * 1024:  # 5MB limit
                logger.warning(f"‚ö†Ô∏è Image too large: {len(response.content)} bytes")
                return None
            
            # TODO: Implement actual OCR processing
            # This is a placeholder implementation for Phase 2
            # Full implementation would use EasyOCR, Tesseract, or PaddleOCR
            
            # Simulated OCR text extraction for testing
            ocr_text = self._simulate_ocr_extraction(image_url)
            
            if ocr_text:
                logger.info(f"‚úÖ OCR extracted {len(ocr_text)} characters")
                return ocr_text
            
            # Placeholder for actual OCR implementation:
            # 
            # import easyocr
            # import cv2
            # import numpy as np
            # from PIL import Image
            # import io
            # 
            # # Convert response content to image
            # image = Image.open(io.BytesIO(response.content))
            # image_array = np.array(image)
            # 
            # # Preprocess image for better OCR results
            # gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)
            # 
            # # Apply image enhancement techniques
            # # - Noise reduction
            # denoised = cv2.medianBlur(gray, 3)
            # 
            # # - Contrast enhancement
            # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            # enhanced = clahe.apply(denoised)
            # 
            # # - Thresholding for better text detection
            # _, thresh = cv2.threshold(enhanced, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            # 
            # # Initialize OCR reader
            # reader = easyocr.Reader(['en'])
            # 
            # # Perform OCR
            # results = reader.readtext(thresh)
            # 
            # # Extract text from results
            # extracted_text = ' '.join([result[1] for result in results if result[2] > 0.5])
            # 
            # return extracted_text if extracted_text.strip() else None
            
        except Exception as e:
            logger.error(f"‚ùå OCR processing failed: {e}")
            return None
    
    def _simulate_ocr_extraction(self, image_url: str) -> Optional[str]:
        """Simulate OCR text extraction for testing purposes"""
        # This is a placeholder that simulates OCR results
        # In a real implementation, this would be replaced by actual OCR
        
        simulated_menu_texts = [
            "APPETIZERS\nBruschetta $8.99\nCalamari Rings $12.99\nStuffed Mushrooms $9.99",
            "MAIN COURSES\nGrilled Salmon $18.99\nChicken Parmesan $16.99\nBeef Tenderloin $24.99",
            "DESSERTS\nTiramisu $7.99\nChocolate Cake $6.99\nIce Cream $4.99"
        ]
        
        # Return a random simulated menu text for testing
        import hashlib
        url_hash = int(hashlib.md5(image_url.encode()).hexdigest()[:8], 16)
        return simulated_menu_texts[url_hash % len(simulated_menu_texts)]
    
    def _extract_items_from_ocr_text(self, ocr_text: str) -> List[Dict[str, Any]]:
        """Extract menu items from OCR-processed text"""
        items = []
        
        try:
            # Split text into lines
            lines = [line.strip() for line in ocr_text.split('\n') if line.strip()]
            
            for line in lines:
                # Skip section headers
                if line.upper() in ['APPETIZERS', 'MAIN COURSES', 'ENTREES', 'DESSERTS', 'BEVERAGES', 'MENU']:
                    continue
                
                # Look for lines with prices (likely menu items)
                price_match = None
                for pattern in self.price_patterns:
                    price_match = re.search(pattern, line)
                    if price_match:
                        break
                
                if price_match:
                    # Extract name and price
                    price = price_match.group(0)
                    name = line.replace(price, '').strip()
                    
                    if len(name) > 2 and len(name) < 100:
                        item = {
                            'name': name,
                            'description': '',
                            'price': price,
                            'raw_text': line,
                            'allergens': [],
                            'extraction_source': 'ocr'
                        }
                        
                        if self._is_valid_menu_item(item):
                            items.append(item)
            
            logger.info(f"üìã Extracted {len(items)} items from OCR text")
            
        except Exception as e:
            logger.error(f"‚ùå OCR text processing failed: {e}")
        
        return items
    
    async def cleanup(self):
        """Clean up browser resources"""
        try:
            if self.page:
                await self.page.close()
            if self.browser:
                await self.browser.close()
            logger.info("üßπ Browser cleanup completed")
        except Exception as e:
            logger.error(f"‚ùå Cleanup failed: {e}")

# Example usage and testing
if __name__ == "__main__":
    async def test_scraper():
        scraper = ProductionMenuScraper(headless=True)
        
        try:
            await scraper.setup_browser()
            
            # Test with a sample URL
            test_url = "https://www.yelp.com/biz/la-crepe-bistro-homer-glen"
            result = await scraper.extract_menu_items(test_url)
            
            print(f"\nüéØ PRODUCTION SCRAPER TEST RESULTS:")
            print(f"Success: {result['success']}")
            print(f"Items extracted: {result['total_items']}")
            print(f"Processing time: {result['processing_time']}s")
            print(f"Extraction method: {result['extraction_method']}")
            print(f"Price coverage: {result['price_coverage']}%")
            
            if result['items']:
                print(f"\nüìã Sample items:")
                for i, item in enumerate(result['items'][:3], 1):
                    print(f"  {i}. {item['name']} - {item.get('price', 'No price')}")
                    if item.get('allergens'):
                        print(f"     Allergens: {', '.join(item['allergens'])}")
            
        finally:
            await scraper.cleanup()
    
    # Run the test
    asyncio.run(test_scraper())
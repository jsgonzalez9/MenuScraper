#!/usr/bin/env python3
"""
Final ML-Enhanced Menu Scraper Test Suite
Comprehensive evaluation with ML features and allergen detection
Target: Achieve 50%+ success rate with enhanced ML capabilities
"""

import asyncio
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
import statistics

from final_ml_menu_scraper import FinalMLMenuScraper

def load_restaurant_data() -> List[Dict[str, str]]:
    """Load restaurant data from JSON file"""
    try:
        with open('restaurant_data.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        # Handle both flat list and nested dictionary structures
        if isinstance(data, list):
            restaurants = data
        elif isinstance(data, dict) and 'restaurants' in data:
            restaurants = data['restaurants']
        else:
            raise ValueError("Invalid restaurant data format")
            
        print(f"ğŸ“Š Loaded {len(restaurants)} restaurants for ML testing")
        return restaurants
        
    except FileNotFoundError:
        print("âŒ restaurant_data.json not found")
        return []
    except Exception as e:
        print(f"âŒ Error loading restaurant data: {e}")
        return []

async def test_final_ml_scraper(sample_size: int = 20) -> Dict[str, Any]:
    """Test the final ML-enhanced scraper with comprehensive metrics"""
    print(f"\nğŸ¤– STARTING FINAL ML-ENHANCED SCRAPER TEST")
    print(f"ğŸ“Š Sample size: {sample_size}")
    print(f"ğŸ¯ Target success rate: 50%+")
    print(f"ğŸ§  ML Features: Enhanced allergen detection, confidence scoring, smart extraction")
    print("=" * 80)
    
    # Load restaurant data
    restaurants = load_restaurant_data()
    if not restaurants:
        print("âŒ No restaurant data available for testing")
        return {}
    
    # Limit sample size
    test_restaurants = restaurants[:sample_size]
    
    # Initialize ML scraper
    scraper = FinalMLMenuScraper(headless=True, timeout=25000)
    
    # Test results storage
    results = {
        'scraper_version': 'Final ML-Enhanced v1.0',
        'test_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'sample_size': len(test_restaurants),
        'success_rate': 0.0,
        'successful_scrapes': 0,
        'total_items_extracted': 0,
        'avg_items_per_success': 0,
        'avg_processing_time': 0,
        
        # ML-specific metrics
        'ml_metrics': {
            'avg_confidence_score': 0,
            'confidence_score_distribution': {
                'high': 0,  # > 0.7
                'medium': 0,  # 0.4-0.7
                'low': 0   # < 0.4
            },
            'extraction_strategy_success': {},
            'quality_assessment_avg': 0,
            'ml_features_usage': {
                'enhanced_css_selectors': 0,
                'ml_text_analysis': 0,
                'structured_data_ml': 0
            }
        },
        
        # Allergen detection metrics
        'allergen_metrics': {
            'total_allergen_detections': 0,
            'allergen_distribution': {},
            'items_with_allergens': 0,
            'allergen_coverage_rate': 0,
            'risk_level_distribution': {
                'high': 0,
                'medium': 0,
                'low': 0
            }
        },
        
        # Quality metrics
        'quality_metrics': {
            'items_with_prices': 0,
            'items_with_descriptions': 0,
            'items_with_categories': 0,
            'items_with_dietary_tags': 0,
            'avg_item_name_length': 0,
            'price_coverage_avg': 0
        },
        
        # Performance metrics
        'performance_metrics': {
            'fastest_scrape': float('inf'),
            'slowest_scrape': 0,
            'total_test_time': 0,
            'avg_items_per_minute': 0
        },
        
        'error_analysis': {},
        'detailed_results': []
    }
    
    start_time = time.time()
    
    try:
        # Setup browser
        if not await scraper.setup_browser():
            print("âŒ Failed to setup ML scraper browser")
            return results
        
        print(f"\nğŸ”„ Testing {len(test_restaurants)} restaurants with ML enhancement...\n")
        
        # Test each restaurant
        for i, restaurant in enumerate(test_restaurants, 1):
            restaurant_name = restaurant.get('name', f'Restaurant {i}')
            url = restaurant.get('url', '')
            
            print(f"[{i:2d}/{len(test_restaurants)}] ğŸ½ï¸  Testing: {restaurant_name}")
            print(f"           ğŸŒ URL: {url}")
            
            # Extract menu items with ML
            result = await scraper.extract_menu_items(url)
            
            # Process results
            processing_time = result.get('processing_time', 0)
            success = result.get('success', False)
            total_items = result.get('total_items', 0)
            extraction_method = result.get('extraction_method', 'unknown')
            ml_features = result.get('ml_features', {})
            
            # Update basic metrics
            if success:
                results['successful_scrapes'] += 1
                results['total_items_extracted'] += total_items
                
                # ML metrics processing
                confidence_scores = ml_features.get('confidence_scores', [])
                if confidence_scores:
                    avg_confidence = sum(confidence_scores) / len(confidence_scores)
                    results['ml_metrics']['avg_confidence_score'] += avg_confidence
                    
                    # Confidence distribution
                    for score in confidence_scores:
                        if score > 0.7:
                            results['ml_metrics']['confidence_score_distribution']['high'] += 1
                        elif score > 0.4:
                            results['ml_metrics']['confidence_score_distribution']['medium'] += 1
                        else:
                            results['ml_metrics']['confidence_score_distribution']['low'] += 1
                
                # Extraction strategy tracking
                strategies = ml_features.get('extraction_strategies_used', [])
                for strategy in strategies:
                    results['ml_metrics']['extraction_strategy_success'][strategy] = \
                        results['ml_metrics']['extraction_strategy_success'].get(strategy, 0) + 1
                    results['ml_metrics']['ml_features_usage'][strategy] = \
                        results['ml_metrics']['ml_features_usage'].get(strategy, 0) + 1
                
                # Quality assessment
                quality_score = ml_features.get('quality_assessment', 0)
                results['ml_metrics']['quality_assessment_avg'] += quality_score
                
                # Allergen analysis
                allergen_summary = result.get('allergen_summary', {})
                for allergen, count in allergen_summary.items():
                    results['allergen_metrics']['allergen_distribution'][allergen] = \
                        results['allergen_metrics']['allergen_distribution'].get(allergen, 0) + count
                    results['allergen_metrics']['total_allergen_detections'] += count
                
                # Item-level analysis
                items = result.get('items', [])
                items_with_allergens = 0
                risk_levels = {'high': 0, 'medium': 0, 'low': 0}
                
                for item in items:
                    # Quality metrics
                    if item.get('price'):
                        results['quality_metrics']['items_with_prices'] += 1
                    if item.get('description') and len(item['description']) > 5:
                        results['quality_metrics']['items_with_descriptions'] += 1
                    if item.get('category'):
                        results['quality_metrics']['items_with_categories'] += 1
                    if item.get('dietary_tags'):
                        results['quality_metrics']['items_with_dietary_tags'] += 1
                    
                    # Allergen metrics
                    if item.get('allergens'):
                        items_with_allergens += 1
                        risk_level = item.get('allergen_risk_level', 'low')
                        risk_levels[risk_level] = risk_levels.get(risk_level, 0) + 1
                    
                    # Name length
                    name_length = len(item.get('name', ''))
                    if name_length > 0:
                        current_avg = results['quality_metrics']['avg_item_name_length']
                        total_items_so_far = results['total_items_extracted']
                        results['quality_metrics']['avg_item_name_length'] = \
                            (current_avg * (total_items_so_far - 1) + name_length) / total_items_so_far
                
                results['allergen_metrics']['items_with_allergens'] += items_with_allergens
                for level, count in risk_levels.items():
                    results['allergen_metrics']['risk_level_distribution'][level] += count
                
                # Price coverage
                price_coverage = result.get('price_coverage', 0)
                results['quality_metrics']['price_coverage_avg'] += price_coverage
                
                print(f"           âœ… Success: {total_items} items extracted")
                print(f"           â±ï¸  Time: {processing_time}s")
                print(f"           ğŸ”§ Method: {extraction_method}")
                print(f"           ğŸ§  ML Quality: {quality_score:.2f}")
                if confidence_scores:
                    print(f"           ğŸ“Š Avg Confidence: {avg_confidence:.2f}")
                if allergen_summary:
                    print(f"           ğŸš¨ Allergens: {sum(allergen_summary.values())} detected")
                
            else:
                error = result.get('error', 'Unknown error')
                results['error_analysis'][error] = results['error_analysis'].get(error, 0) + 1
                print(f"           âŒ Failed: {error}")
                print(f"           â±ï¸  Time: {processing_time}s")
            
            # Performance tracking
            results['performance_metrics']['fastest_scrape'] = min(
                results['performance_metrics']['fastest_scrape'], processing_time
            )
            results['performance_metrics']['slowest_scrape'] = max(
                results['performance_metrics']['slowest_scrape'], processing_time
            )
            
            # Store detailed result
            detailed_result = {
                'restaurant_name': restaurant_name,
                'url': url,
                'scraping_success': success,
                'total_items': total_items,
                'processing_time': processing_time,
                'extraction_method': extraction_method,
                'ml_features': ml_features,
                'allergen_summary': result.get('allergen_summary', {}),
                'price_coverage': result.get('price_coverage', 0),
                'error': result.get('error')
            }
            results['detailed_results'].append(detailed_result)
            
            print()  # Empty line for readability
            
            # Small delay between requests
            await asyncio.sleep(1)
    
    finally:
        await scraper.cleanup()
    
    # Calculate final metrics
    total_test_time = time.time() - start_time
    results['performance_metrics']['total_test_time'] = round(total_test_time, 2)
    
    if results['successful_scrapes'] > 0:
        results['success_rate'] = round(
            (results['successful_scrapes'] / len(test_restaurants)) * 100, 1
        )
        results['avg_items_per_success'] = round(
            results['total_items_extracted'] / results['successful_scrapes'], 1
        )
        
        # ML metrics averages
        results['ml_metrics']['avg_confidence_score'] = round(
            results['ml_metrics']['avg_confidence_score'] / results['successful_scrapes'], 3
        )
        results['ml_metrics']['quality_assessment_avg'] = round(
            results['ml_metrics']['quality_assessment_avg'] / results['successful_scrapes'], 2
        )
        
        # Quality metrics averages
        results['quality_metrics']['price_coverage_avg'] = round(
            results['quality_metrics']['price_coverage_avg'] / results['successful_scrapes'], 1
        )
        
        # Allergen coverage rate
        if results['total_items_extracted'] > 0:
            results['allergen_metrics']['allergen_coverage_rate'] = round(
                (results['allergen_metrics']['items_with_allergens'] / results['total_items_extracted']) * 100, 1
            )
    
    # Calculate average processing time
    if results['detailed_results']:
        total_processing_time = sum(r['processing_time'] for r in results['detailed_results'])
        results['avg_processing_time'] = round(
            total_processing_time / len(results['detailed_results']), 2
        )
        
        # Items per minute
        if total_test_time > 0:
            results['performance_metrics']['avg_items_per_minute'] = round(
                (results['total_items_extracted'] / total_test_time) * 60, 1
            )
    
    # Fix infinite value for fastest scrape
    if results['performance_metrics']['fastest_scrape'] == float('inf'):
        results['performance_metrics']['fastest_scrape'] = 0
    
    return results

def save_results(results: Dict[str, Any], filename: str = None) -> str:
    """Save test results to JSON file"""
    if not filename:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'final_ml_test_results_{timestamp}.json'
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“ Results saved to: {filename}")
        return filename
    except Exception as e:
        print(f"âŒ Failed to save results: {e}")
        return ""

def print_comprehensive_ml_summary(results: Dict[str, Any]):
    """Print comprehensive ML test summary"""
    print("\n" + "=" * 80)
    print("ğŸ¤– FINAL ML-ENHANCED SCRAPER TEST RESULTS SUMMARY")
    print("=" * 80)
    
    # Basic metrics
    print(f"\nğŸ“Š CORE METRICS:")
    print(f"   ğŸ¯ Success Rate: {results['success_rate']}% ({results['successful_scrapes']}/{results['sample_size']})")
    print(f"   ğŸ“‹ Total Items: {results['total_items_extracted']}")
    print(f"   ğŸ“ˆ Avg Items/Success: {results['avg_items_per_success']}")
    print(f"   â±ï¸  Avg Processing Time: {results['avg_processing_time']}s")
    
    # Performance assessment
    target_success_rate = 50.0
    if results['success_rate'] >= target_success_rate:
        print(f"   ğŸ‰ TARGET ACHIEVED! ({results['success_rate']}% >= {target_success_rate}%)")
    else:
        gap = target_success_rate - results['success_rate']
        print(f"   âš ï¸  Target missed by {gap}% ({results['success_rate']}% < {target_success_rate}%)")
    
    # ML-specific metrics
    ml_metrics = results['ml_metrics']
    print(f"\nğŸ§  ML ENHANCEMENT METRICS:")
    print(f"   ğŸ“Š Avg Confidence Score: {ml_metrics['avg_confidence_score']:.3f}")
    print(f"   ğŸ¯ Quality Assessment: {ml_metrics['quality_assessment_avg']:.2f}")
    
    # Confidence distribution
    conf_dist = ml_metrics['confidence_score_distribution']
    total_conf_items = sum(conf_dist.values())
    if total_conf_items > 0:
        print(f"   ğŸ“ˆ Confidence Distribution:")
        print(f"      â€¢ High (>0.7): {conf_dist['high']} ({conf_dist['high']/total_conf_items*100:.1f}%)")
        print(f"      â€¢ Medium (0.4-0.7): {conf_dist['medium']} ({conf_dist['medium']/total_conf_items*100:.1f}%)")
        print(f"      â€¢ Low (<0.4): {conf_dist['low']} ({conf_dist['low']/total_conf_items*100:.1f}%)")
    
    # Extraction strategies
    if ml_metrics['extraction_strategy_success']:
        print(f"   ğŸ”§ Successful Extraction Strategies:")
        for strategy, count in ml_metrics['extraction_strategy_success'].items():
            print(f"      â€¢ {strategy}: {count} successes")
    
    # Allergen detection metrics
    allergen_metrics = results['allergen_metrics']
    print(f"\nğŸš¨ ALLERGEN DETECTION METRICS:")
    print(f"   ğŸ” Total Detections: {allergen_metrics['total_allergen_detections']}")
    print(f"   ğŸ“Š Items with Allergens: {allergen_metrics['items_with_allergens']}")
    print(f"   ğŸ“ˆ Allergen Coverage: {allergen_metrics['allergen_coverage_rate']}%")
    
    # Allergen distribution
    if allergen_metrics['allergen_distribution']:
        print(f"   ğŸ·ï¸  Allergen Types Detected:")
        for allergen, count in sorted(allergen_metrics['allergen_distribution'].items(), key=lambda x: x[1], reverse=True):
            print(f"      â€¢ {allergen}: {count} occurrences")
    
    # Risk level distribution
    risk_dist = allergen_metrics['risk_level_distribution']
    total_risk_items = sum(risk_dist.values())
    if total_risk_items > 0:
        print(f"   âš ï¸  Risk Level Distribution:")
        print(f"      â€¢ High Risk: {risk_dist['high']} ({risk_dist['high']/total_risk_items*100:.1f}%)")
        print(f"      â€¢ Medium Risk: {risk_dist['medium']} ({risk_dist['medium']/total_risk_items*100:.1f}%)")
        print(f"      â€¢ Low Risk: {risk_dist['low']} ({risk_dist['low']/total_risk_items*100:.1f}%)")
    
    # Quality metrics
    quality_metrics = results['quality_metrics']
    print(f"\nğŸ“ˆ QUALITY METRICS:")
    total_items = results['total_items_extracted']
    if total_items > 0:
        price_pct = (quality_metrics['items_with_prices'] / total_items) * 100
        desc_pct = (quality_metrics['items_with_descriptions'] / total_items) * 100
        cat_pct = (quality_metrics['items_with_categories'] / total_items) * 100
        diet_pct = (quality_metrics['items_with_dietary_tags'] / total_items) * 100
        
        print(f"   ğŸ’° Items with prices: {quality_metrics['items_with_prices']} ({price_pct:.1f}%)")
        print(f"   ğŸ“ Items with descriptions: {quality_metrics['items_with_descriptions']} ({desc_pct:.1f}%)")
        print(f"   ğŸ·ï¸  Items with categories: {quality_metrics['items_with_categories']} ({cat_pct:.1f}%)")
        print(f"   ğŸ¥— Items with dietary tags: {quality_metrics['items_with_dietary_tags']} ({diet_pct:.1f}%)")
        print(f"   ğŸ“ Avg name length: {quality_metrics['avg_item_name_length']:.1f} chars")
        print(f"   ğŸ’° Avg price coverage: {quality_metrics['price_coverage_avg']:.1f}%")
    
    # Performance metrics
    perf_metrics = results['performance_metrics']
    print(f"\nâš¡ PERFORMANCE METRICS:")
    print(f"   ğŸƒ Fastest scrape: {perf_metrics['fastest_scrape']}s")
    print(f"   ğŸŒ Slowest scrape: {perf_metrics['slowest_scrape']}s")
    print(f"   ğŸ• Total test time: {perf_metrics['total_test_time']}s")
    print(f"   ğŸ“Š Items per minute: {perf_metrics['avg_items_per_minute']}")
    
    # Error analysis
    if results['error_analysis']:
        print(f"\nâŒ ERROR ANALYSIS:")
        for error, count in sorted(results['error_analysis'].items(), key=lambda x: x[1], reverse=True):
            percentage = (count / results['sample_size']) * 100
            print(f"   â€¢ {error}: {count} occurrences ({percentage:.1f}%)")
    
    # Comparison with previous versions
    print(f"\nğŸ”„ COMPARISON WITH PREVIOUS VERSIONS:")
    previous_results = {
        'Enhanced Scraper': 40.0,
        'Advanced Scraper': 30.0,
        'Final Optimized': 40.0,
        'Production Scraper': 0.0,
        'ML Enhanced': 0.0,
        'Enhanced Allergen': 0.0
    }
    
    current_rate = results['success_rate']
    best_previous = max(previous_results.values())
    
    for version, rate in previous_results.items():
        print(f"   {version}: {rate}% success rate")
    
    print(f"   Final ML-Enhanced: {current_rate}% success rate")
    
    if current_rate > best_previous:
        improvement = current_rate - best_previous
        print(f"   ğŸ‰ NEW BEST! Improved by {improvement}% over previous best")
    elif current_rate == best_previous:
        print(f"   ğŸ¤ MATCHES best previous performance")
    else:
        decline = best_previous - current_rate
        print(f"   âš ï¸  Final ML is {decline}% below best previous performance")
    
    # ML-specific recommendations
    print(f"\nğŸš€ ML-ENHANCED NEXT STEPS:")
    if results['success_rate'] >= 50:
        print(f"   â€¢ ğŸ¯ TARGET ACHIEVED! Deploy ML-enhanced scraper")
        print(f"   â€¢ ğŸ§  Fine-tune ML models with collected data")
        print(f"   â€¢ ğŸ“ˆ Implement continuous learning pipeline")
    else:
        print(f"   â€¢ ğŸ” Analyze ML confidence patterns for improvement")
        print(f"   â€¢ ğŸ§  Enhance allergen detection algorithms")
        print(f"   â€¢ ğŸ“Š Collect more training data for ML models")
        print(f"   â€¢ ğŸ¯ Implement ensemble extraction methods")
    
    print(f"   â€¢ ğŸš¨ Expand allergen database with user feedback")
    print(f"   â€¢ ğŸ“± Integrate with AllergySavvy-style ML pipeline")
    print(f"   â€¢ ğŸ”„ Implement real-time model updates")
    
    print("\n" + "=" * 80)
    print("âœ… Final ML-Enhanced test completed successfully!")

async def main():
    """Main test execution"""
    # Run comprehensive ML test
    results = await test_final_ml_scraper(sample_size=20)
    
    if results:
        # Print summary
        print_comprehensive_ml_summary(results)
        
        # Save results
        filename = save_results(results, 'final_ml_test_results_n20.json')
        print(f"ğŸ“ Detailed results saved to: {filename}")
    else:
        print("âŒ ML test failed to complete")

if __name__ == "__main__":
    asyncio.run(main())